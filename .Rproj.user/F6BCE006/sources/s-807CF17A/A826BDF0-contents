---
title: "Right contex effects and adaptation of stop consonants (in English)"
subtitle: ""
author: "Joselyn Rodriguez"
institute: "Rutgers University"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: [default, rutgers, rutgers-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

<!-- 

#### TODO 


1. Relevel variables.
  - the base factor should be /t/-biasing
  - make sure VOT is centered correctly




-->


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)

```
```{r, include = FALSE}
library("tidyverse")
library("brms")
library("tidybayes")
library("lme4")
library("lmerTest")
library("psych")

# load some data that i'll need real quick

temp <- read_csv("data/batch1-test.csv") 

test <- temp %>% 
          mutate(resp_t = as.numeric(resp_t))

test %>% 
  group_by(workerId) %>% 
  summarize(average = mean(resp_t), sd = sd(resp_t))

pre_test <- test %>% 
  filter(block == "pre")
nrow(pre_test)

post_test <- test %>% 
  filter(block == "post")
nrow(post_test)

exposure <- read_csv("data/batch1-exposure.csv")
```


# Jumping right in: The study
 
- Given previous research finding that when an ambiguous segment is encountered, listeners do not immediately try to disambiguate it to determine a lexical item, but maintain uncertainty until disambiguating information is encountered, we were interested in 
  - (1) what information is available 
  - (2) whether this finer-grained information is available for updating/learning
--

- Moving on...

---

# Jumping right in: The study

- In order to test whether or not this semantic information that is available after encountering an ambiguous item is available to use in updating of representational information, we used a **perceptual learning** paradigm.
  - cite like Norris et al (2003) and mcqueen maybe?

--

### Perceptual learning paradigms: the gist


Pre-test --> Recalibration of some sort --> Post-test (yay! learning hopefully)

<!-- insert pic of the process -->

---

# Methods 

- for this study, the methods were similar to other perceptual learning paradigms but instead of having a recalibration take place through lexical disambiguation, we used semantic disambiguation. 

- ex:

"After the tent in the | campgrounds collapsed, we went to a hotel."


- the critical sentences were taken from a previous study examining semantic right context effects on sentence comprehension and were re-recorded by one female speaker of American English (not me)
  - Connine et al (1991)
---

# Methods
#### Pre-test

- stimuli for the pre-test consisted of a VOT continuum ranging from 15-85ms at 5ms intervals and were presented as "the tent" and "the dent" with only the VOT of the /t/ and /d/ differing between the two
- this lasted about 11 minutes 
--

#### Exposure 
- sentences mentioned above. Each were "short lag" sentences such that the disambiguating information was provided within 3-5 syllables of encountering the ambiguous word
- participants were asked to respond whether they heard "tent" or "dent" used in the sentence
  - this was both as an attention check as well as a way of comparing these findings to previous studies
    - i.e., Connine et al (1991)
--

#### Pos-test
- identical to the pre-test


---
# Analysis (the important part)
## Bayesian vs. Frequentist analysis of the pre/post-test data

- utilized a Bayesian multi-level model (using brms) and a Frequentist model using (lme4)

- we are utilizing a multi-level logistic regression because...
  - the data is looking at the proportion of /t/-responses (for pre/post tests)
  - and accuracy along with proportion /t/-responses (for exposure)
- multi-level because...
  - we're interested in group-level effects since it's repeated measures and because we want to look at within-participant effects
  
---
# Analysis
## Bayesian vs. Frequentist analysis of the pre/post-test data

.Large[$$\hat{y} = \alpha + {\beta}{X} + {u}{Z} + \epsilon $$]

- recall the equation for MLM


---
# Analysis
# Test data

- Frequentist approach: 

- in order to compare the pre- and post-test, I used lme4 to run the following model: 
  - `glmer(resp_t ~ block * condition * vot + (block | workerId) + (1 | stimulus), data = test, family = "binomial"(link = "logit")`
  
  - taking this apart:
    - glmer: a generali**zed** multi-level regression model 
    - outcome variable: proportion of /t/ responses
    - fixed effects: block (pre vs. post), condition (tent or dent-biasing), and vot
    - random effects: workerId, item, block
      - specifically, random intercepts by item and subject and by subject random slopes for blocks

```{r frequentist}

# convert to probability: logit2prob <- function(logit){
#   odds <- exp(logit)
#   prob <- odds / (1 + odds)
#   return(prob)
# }
# infinite thanks to: https://sebastiansauer.github.io/convert_logit2prob/


freq_analysis_int <- glm(resp_t ~ 1, data = test, family = "binomial"(link = "logit"))
summary(freq_analysis_int)
mean(test$resp_t)


freq_analysis_1 <- glmer(resp_t ~ block * condition * vot +
                                  (1 | workerId),
                                  data = test, family = "binomial"(link = "logit"))

freq_analysis_2 <- glmer(resp_t ~ block * condition * vot +
                                  (1 | workerId) +
                                  (1 | stimulus),
                                  data = test, family = "binomial"(link = "logit"), 
                                  control=glmerControl(optimizer="bobyqa",
                                  optCtrl=list(maxfun=2e5)))

freq_analysis_full <- glmer(resp_t ~ block * condition * vot +
                                  (block | workerId) +
                                  (1 | stimulus),
                                  data = test, family = "binomial"(link = "logit"), 
                                  control=glmerControl(optimizer="bobyqa",
                                  optCtrl=list(maxfun=2e5)))

summary(freq_analysis_full)
anova(freq_analysis_1, freq_analysis_2 , freq_analysis_full, test = "Chisq") # well it looks like including stimulus is better?

```



---
# Analysis
# Pre-data

- Bayesian approach: 

- alright, this is very similar, but we're going to make a few adjustments. 
  - `brm(resp_t ~ block * condition * vot + (block | workerId) + (1 | stimulus), data = test, family = "binomial"(link = "logit")`
  
```{r bayesian}

bayes_analysis_int <- brm(resp_t ~ 1, data = test, family = "binomial"(link = "logit"))
summary(bayes_analysis_int)
mean(test$resp_t)


bayes_analysis_1 <- brm(resp_t ~ block * condition * vot +
                                  (1 | workerId),
                                  data = test, family = "binomial"(link = "logit"))

bayes_analysis_2 <- brm(resp_t ~ block * condition * vot +
                                  (1 | workerId) +
                                  (1 | stimulus),
                                  data = test, family = "binomial"(link = "logit"))
summary(bayes_analysis_1)
bayes_analysis_full <- brm(resp_t ~ block * condition * vot +
                                  (block | workerId) +
                                  (1 | stimulus),
                                  data = test, family = "binomial"(link = "logit"))

summary(bayes_analysis_full)
anova(bayes_analysis_1, bayes_analysis_2 , bayes_analysis_full, test = "Chisq") # well it looks like including stimulus is better?

```




