---
title             : "Right Context Effects and Learning in Speech Perception"
shorttitle        : "Right Context and Learning"

author: 
  - name          : "Joselyn Rodriguez"

bibliography      : ["citations.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
# linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---


```{r setup, include = FALSE}
# APA markdown setup
library("papaja")
r_refs("citations.bib")
```

```{r analysis-preferences}
knitr::opts_chunk$set(cache.extra = knitr::rand_seed, echo = FALSE, fig.width=5, fig.height=3, results = 'hide', message = FALSE, warning = FALSE)

library(tidyverse)
library(readr)
library(lme4)
library(sjPlot)
library(mdthemes)

options(digits=2)

```

```{r old import (ignore)}
# test is the data that Dave cleaned - since I want to include RT here, I can't use this plus it's good practice :(
test <- read_csv("data/batch1-test.csv") %>%
        mutate(block = factor(block, levels=c("pre", "post")),
               resp_t = as.numeric(resp_t))  %>%
        mutate(condition = factor(condition, levels = c("tent-biasing", "dent-biasing")))

exposure <- read_csv("data/batch1-exposure.csv") %>%
            mutate(resp_t = as.numeric(resp_t))
```


```{r cleaning data}

# I'm sorry this is a mess but so was this csv :(
questionnaire <- read_csv("data/batch1.csv", col_types = cols(sender = col_character(), 
                                                         workerId = col_character(), audioequip = col_character(), 
                                                         sex = col_character(), ethnicity = col_character(), 
                                                         race = col_character(), raceother = col_character(), 
                                                         born = col_character(), parent = col_character(), 
                                                         lang = col_character(), comments = col_character(), 
                                                         age = col_number())) %>% 
                select("sender", "workerId", "audioequip":"comments") %>% 
                filter(sender == "Quesionnaire Form")


# block_id, trial, resp are all created variables
data_full <- read_csv("data/batch1.csv", col_types = cols(sender = col_character(), 
                                                         workerId = col_character(), audioequip = col_character(), 
                                                         sex = col_character(), ethnicity = col_character(), 
                                                         race = col_character(), raceother = col_character(), 
                                                         born = col_character(), parent = col_character(), 
                                                         lang = col_character(), comments = col_character(), 
                                                         age = col_number())) %>% 
                select("workerId","condition","stimulus","response","sender","sender_id","block", "duration", "item", "item_block", "time") %>% 
                filter(sender == "Sounds" | sender == "Response") %>% 
                mutate(block_id = substr(sender_id,1,1),
                       resp_t = case_when(substr(response, 1,1) == "t" ~ "TRUE",
                                            substr(response, 1,1) == "d" ~ "FALSE"),
                       vot = substr(stimulus, 11, 12))

# this is the data for just the pre- and post-tests 
data_test <- data_full %>% 
                select("workerId","condition","stimulus","response","sender","sender_id","duration", "block_id") %>% 
                mutate(block = case_when(block_id=="6"~ "pre",
                                         block_id=="10"~ "post")) %>% 
                filter(block == "pre" | block == "post") %>% 
                filter(sender == "Sounds")

data_exposure <- data_full %>% 
                  select("workerId","condition","response","sender","block_id","duration", "item","time") %>% 
                  filter(block_id == "8") %>% 
                  filter(sender == "Response") %>% 
                  mutate(bias = case_when(str_detect(item, "tent")~ "tent",
                                          str_detect(item, "dent")~ "dent"))

# write output to csvs

# write_csv(data_full, "data/data_full.csv")
# write_csv(data_test, "data/data_test.csv")
# write_csv(data_exposure, "data/data_exposure.csv")
# write_csv(questionnaire, "data/questionnaire.csv")
```

# Introduction

<!--

The important questions here are surrounding ambiguity. The problem is that to have a discussion about ambiguity, we're also going to have to talk about prediction and uncertainty. For the garden path sentence, you can't really say it's ambiguous (can you?) because it's not actually ambiguous until you reach a certain point in the sentence and your parse fails. It's not ambiguous in the same way that a word could be semantically or acoustically ambiguous between multiple different words (and you're aware of that ambiguity). This is all to say that I think the question needs to be framed slightly differently. 

A better way to think about this here is about how long ambiguity is maintained and how ambiguous a word needs to be in order for ambiguity to be maintained in the first place (this was the whole deal with that Bicknell et al paper)

-->

Ambiguity is ever present aspect of speech comprehension that is present at multiple levels including phonetic, lexical, syntactic, semantic, and pragmatic. A classic example of syntactic ambiguity are  "garden path sentences" which display uncertainty that arises due to a error in syntactic parsing early in the sentence that leads to a breakdown in comprehension - i.e., the "garden path". This can be seen in a sentence like the following: "the horse raced past the barn *fell*". In order to ascertain the correct meaning of the sentence, the structure must be disambiguated at a specific point in the sentence at which the listener is able to re-parse the sentence correctly. On the other than, lexical ambiguity can be illustrated through homophony, in which one word may relate to multiple meanings depending on the context. 

In the acoustic-phonetic domain, ambiguity that is present in the signal can be due to several factors including variation in pronunciation attributed to physical factors (vocal tract length, mouth shape) as well as socio-economic factors (dialect, gender, economic status). The amount of variation present in the signal often leads to scenarios in which the identity of a word or segment is ambiguous without further information. An example of this is best illustrated across spectral center of gravity in fricatives (like /s/ and /sh/), where it's possible that a speaker's production of /s/ may overlap _entirely_ with a different speaker's production of /sh/, making identification of the segment ambiguous between the two without further contextual information (for example, one person's production of "ship" would overlap completely with a different talker's production of "sip"). 

Because such uncertainty is a persistent issue in communication that must be resolved by the perceptual system in order to make accurate inferences during comprehension, several theories have been proposed to account for it. Many of these theories have assumed that when ambiguous information is encountered, it is processed immediately and passed to higher levels of representation and all remaining uncertainty regarding category membership is discarded [@christiansen_now-or-never_2016]. While such "chunk-and-pass" models of speech perception have received influence within the field, evidence from studies in speech perception have shown that ambiguous information later in the sentence is still capable of influencing previous interpretation, suggesting that gradient information in the speech signal is not processed immediately and is at least available for backward influence in the local right-context. 

For example, while previous accounts of speech recognition have assumed that identification of a word occurs quickly after encountering it - within 200ms of word onset -   [@marslen-wilson_access_1989], research from @Connineetal1991 and @szostak_prolonged_2013 suggests that words are not processed immediately upon presentation, but rather that this ambiguous representation is _maintained_ in memory until disambiguating information is encountered in the signal. Surprisingly, the results of @Connineetal1991 suggested that this information was maintained for a considerable amount of time - up to seven syllables *following* exposure to the ambiguous target, in the right context of the target word. Thus, in a sentence in which an ambiguous target ('?ent') is encountered, this ambiguity can be maintained for up to seven syllables as the sentence progresses until a disambiguating cue is reached ('mountain') such as in the example: "Because the _?ent_ was so hard to find on the _mountain_, we had to point it out." In this example sentence, the segment ambiguous between a /d/ or /t/ is unable to be resolved at the word-level because either of these realizations would be acceptable within that context because "tent" and "dent" are both real words of English. 

The listener is unable to determine whether the correct reading of the ambiguous word is "tent" or "dent" until they encounter the word "mountain" which allows the listener to realize the talker is discussing an outdoor scenario in which a discussion of "tents" is more likely than "dents". While these previous studies have shown that this ambiguity can be maintained until disambiguating information is acquired rather than immediately determined from the available acoustic input, it is still unknown whether this effect is strong enough to facilitate learning. Previous studies of segmental ambiguity have found that lexical disambiguation of an ambiguous target segment induces learning (i.e., recalibration) of that target segment. For example, take into consideration /d/ and /t/ in the following words: croco[d]ile" and "cafe[t]eria". 

Since these segments are unambiguously /t/ or /d/ within the word (since the word "crocotile" doens't exist), the ambiguous segment within this lexical context serves to shift the boundary of the assumed category (/d/ for "crocodile") with respect to the ambiguous target. However, whether such recalibration effects will occur in a situation such as lexical ambiguity where the disambiguating semantic context occurs significantly later in the sentence is an open question.

<!--
  You know, now that I'm thinking about this more - I don't think it's possible to really make this kind of claim. 
-->

It is possible that when encountering an ambiguous word that could have multiple lexical realizations, both of those words are activated and one of them is immediately accepted and given all the available information, one is chosen and moved to different levels of representation as processing continues. On the other hand, it's possible that as ambiguous information is encountered, multiple words are activated in the lexicon and the listener maintains gradient activation of these words after which one is identified only after disambiguating evidence is encountered as the sentence proceeds. If gradient information is being maintained at a lower level such as the segmental level, then it's possible that disambiguation of the word would also lead to recalibration of that ambiguous phonetic segment. 

This is supported by recent results from @burchill_maintaining_2018 which suggest that it is possible that information provided after ambiguous information can guide learning of speech encountered previously in the signal. In this study, learning of a foreign accent was facilitated by subtitles provided up to six seconds after subjects heard the accented speech, suggesting that not only is gradient information maintained, but that phonetic adaptation given new information later in the signal is possible. Whether or not this learning can also be facilitated by indirectly solely through semantic information is an open question.

<!--
  added in some information about response times, but honestly i dont think this reponse time is nearly accurate enough to be making any claims at all lol
-->

Therefore, the current project addresses the question of whether learning of segmental information can be facilitated through semantic information present later in a sentence in the right context using a perceptual learning paradigm. The hypothesis is that if segmental information is being maintained during processing, then the disambiguation in the right context may be able to facilitate recalibration of a target /t/-like segment depending on the semantic biasing context. If this is indeed the case, we would expect to see an increase in the proportion of /t/-responses in the tent-biasing condition and decrease in /t/-responses in the dent-biasing condition. Additionally, response times for the categorization functions are expected to be larger for more ambiguous stimuli. Thus, the categorization should be slower towards the center of the continuum for the categorization tasks as well as for the more ambiguous items present in the exposure phase. This would suggest that the participants are indeed perceiving the stimuli as ambiguous. 

This paradigm consists of a pre-exposure block in which participants respond to stimuli from a tent-dent continuum to get a baseline of their prior categorization followed by an exposure phase in which subjects hear 40 sentences containing a target word "?ent" within a sentence biasing participants towards "tent" or "dent" interpretations. 


# Methods

This study followed standard procedures for conducting perceptual learning experiments online and was approved by the Institutional Review Board at Rutgers University. 


### Participants

```{r participant data}
num_part = nrow(questionnaire)
avg_age = mean(questionnaire$age)
med_age = median(questionnaire$age)
sd_age = sd(questionnaire$age)
num_fem = count(questionnaire %>% filter(sex == "Female"))
num_male = count(questionnaire %>% filter(sex == "Male"))
hisp = count(questionnaire %>% filter(ethnicity == "Hisp"))
eng = count(questionnaire %>% filter(parent == "yes"))
audio = table(questionnaire$audioequip) 
```

60 participants total (23 Females; 1 unknown) took part in the experiment. The average age was 40.63 years (*M*: `r avg_age`; *Mdn* = `r med_age`; *SD* = `r sd_age`). 57 participants indicated that English was the language spoken at home (other = 2, NA = 1). Only one participant indicated what other language was spoken other than English (Russian). These participants were included as their language background isn't expected to have a major effect in this study since we are looking at a within-subject effect and even if they were utilizing categorization function from a different language, we would still expect to see a shift in their categorization function following exposure. 

Participants were recruited through Amazon's Mechanical Turk and compensated for their time at a rate of $5/30 minutes. The experiment was created using Lab.js and lasted around 25 minutes but participants were given up to an hour to complete it [@henninger_felix_2020_3953072]. Since the experiment was conducted online and required listening to auditory stimuli, prior to beginning the participants were told to use headphones and completed a quick screening utilizing phase cancellation such that detection of loudness of spatially distributed tones was only possible if they were wearing headphones. Participants were thus only able to proceed to the experiment if they achieved above 66% accuracy in identifying the softest auditory tone. Participants were only able to proceed with the experiment if they passed the headphone check. However, participants were able to then put on headphones and retake the headphone check in order to proceed with the experiment. Therefore, all data presented here are from participants who passed this check. As this experiment is performed online, additional information about the quality of their headphones was collected from participants voluntary in the questionnaire -- the majority of the participants who responded to the questionnaire used in-ear headphones costing less than \$30 (N = 27) or \$100 (N = 13). Less than half of the participants used over-the-ear headphones (N = 17). While the majority of participants were using potentially lower-quality headphones, this is not expected to impact the results.

The experiment consisted of three tasks: two categorization tasks on a tent-dent continuum and an exposure phase which exposed participants to a specific biasing context. Participants were randomly assigned to only one of two conditions: tent-biasing and dent-biasing. Both conditions received the same pre and post tests, but received different biasing contexts for ambiguous stimuli in the exposure phase. After the participants completed the experiment, they were asked to take part in a voluntary questionnaire.

### Materials 

The stimuli for this experiment consisted of a synthesized tent-dent continuum with VOT (voice onset time) for the stop consonants ranging from 10 to 85 ms in 5ms intervals for a total of 16 tokens. The exposure sentences consisted of sentences produced by a single female talker. These sentences were new recordings of the same sentences used in a previous study examining right context effects [@Connineetal1991]. While it has indeed been shown that ambiguity is maintained in sentences up to seven syllables after target exposure, for the purposes of the current study, only short lag sentences in which the disambiguating information appeared approximately three syllables following the target word were used in order to maximize possible learning effects.

Exposure sentences consisted of 20 unique sentences in which a target word appeared at the beginning of the sentence followed by some disambiguating contextual information within approximately three syllables following this target word. The following is an example sentence:

(1) Since the **tent** at the **camp** was removed, we were able to leave.

Half of the sentences contained an ambiguous segment at 50ms while the other half were canonical given the contextual information such that the biasing context matched the VOT of the target segment. For example, in a tent-biasing condition, a normal /d/-biasing context was matched with an acoustically canonical production of /d/ at 10ms while the /t/-biasing sentences were matched with the ambiguous /t/ production at 50ms. This ensured that participants received both ends of the possible range of VOT for a given ambiguous segment. Thus, participants were exposed the the "full range" of the talker in a given condition. To create the exposure sentences, ambiguous segments were synthesized from natural productions by a female American English talker using Praat [@Boersma2009] and spliced onto "-ent" endings that were cut back into the full sentence, creating a naturalistic full production of each sentence. 

### Pre and Post Continuum tasks

The first and final task that the participants completed was a categorization of a tent-dent continuum. As discussed above, the stimuli for this task consisted of a continuum with VOT ranging from 10ms (/d/-like) to 85ms (/t/-like), synthesized from tokens produced by a single female talker. These 16 items were repeated in 8 blocks for a total of 128 tokens and lasted approximately 7 minutes. During this task, participants heard a tent-dent token while seeing "tent" or "dent" displayed on the left and right hand sides of their computer screen and were prompted to respond using the "F" and "J" keys for "tent" or "dent" respectively followed by a fixation cross and ISI of 500ms. Response times were measured as the time it took participants to respond after the stimulus was displayed on the screen. After completion of the pre-continuum task, participants were given the option of taking a short break and then began the exposure phase. The participants completed an identical continuum task again following completion of the exposure phase. Participants were not able to respond before hearing the entirety of the stimulus.

### Exposure Phase

In the exposure phase, participants in both conditions first saw a fixation cross for the duration of the sentence as they heard it and were then prompted to respond whether they heard "tent" or "dent" in the sentence by using the 'f' or 'j' key respectively. Participants were not allowed to respond until after the sentence had completely finished playing. Response times for the exposure phase were recorded as the time it took participants to respond after the Requiring a response served two purposes in this task. First, it was a way of ensuring participants' attention throughout the task and second, it was a way of testing whether participants were indeed perceiving the items as expected given previous research regarding right context effects. As such, we would expect that the responses for this section would follow previous patterns showing response influenced by their biasing context such that the responses are semantically consistent with the right context. The exposure phase consisted of 20 unique sentences with 3 repetitions for a total of 80 tokens and lasted approximately 11 minutes. 



# References 

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
<div id="refs" custom-style="Bibliography"></div>
\endgroup
