---
title: "Right contex effects and adaptation of stop consonants (in English)"
subtitle: ""
author: "Joselyn Rodriguez"
institute: "Rutgers University"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: [default, rutgers, rutgers-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

<!-- 

#### TODO 


1. Relevel variables.
  - the base factor should be /t/-biasing
  - make sure VOT is centered correctly
  
2. Calculate Rope

3. Plot posterior estimates

4. Actually set your priors





-->


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)

```

```{r, include = FALSE}
library(tidyverse)
library(brms)
library(tidybayes)
library(lme4)
library(lmerTest)
library(psych)
library(bayesplot)
library(ProbBayes)


# load some data that i'll need real quick

temp <- read_csv("data/batch1-test.csv") 

# releveld w/pre:tent-biasing as the intercept
test <- temp %>% 
      mutate(block = factor(block, levels=c("pre", "post")),
             resp_t = as.numeric(resp_t))  %>%
      mutate(condition = factor(condition, levels = c("tent-biasing", "dent-biasing"))) %>% 
      mutate(vot = (vot - mean(vot))) # centering

test %>% 
  group_by(workerId) %>% 
  summarize(average = mean(resp_t), sd = sd(resp_t))

pre_test <- test %>% 
  filter(block == "pre")
nrow(pre_test)

post_test <- test %>% 
  filter(block == "post")
nrow(post_test)

exposure <- read_csv("data/batch1-exposure.csv")
```


# Jumping right in: The study
 
- Given previous research finding that when an ambiguous segment is encountered, listeners do not immediately try to disambiguate it to determine a lexical item, but maintain uncertainty until disambiguating information is encountered, we were interested in 
  - (1) what information is available 
  - (2) whether this finer-grained information is available for updating/learning
--

- Moving on...

---

# Jumping right in: The study

- In order to test whether or not this semantic information that is available after encountering an ambiguous item is available to use in updating of representational information, we used a **perceptual learning** paradigm.
  - cite like Norris et al (2003) and mcqueen maybe?

--

### Perceptual learning paradigms: the gist


Pre-test --> Recalibration of some sort --> Post-test (yay! learning hopefully)

<!-- insert pic of the process -->

---

# Methods 

- for this study, the methods were similar to other perceptual learning paradigms but instead of having a recalibration take place through lexical disambiguation, we used semantic disambiguation. 

- ex:

"After the tent in the | campgrounds collapsed, we went to a hotel."


- the critical sentences were taken from a previous study examining semantic right context effects on sentence comprehension and were re-recorded by one female speaker of American English (not me)
  - Connine et al (1991)
---

# Methods
#### Pre-test

- stimuli for the pre-test consisted of a VOT continuum ranging from 15-85ms at 5ms intervals and were presented as "the tent" and "the dent" with only the VOT of the /t/ and /d/ differing between the two
- this lasted about 11 minutes 
--

#### Exposure 
- sentences mentioned above. Each were "short lag" sentences such that the disambiguating information was provided within 3-5 syllables of encountering the ambiguous word
- participants were asked to respond whether they heard "tent" or "dent" used in the sentence
  - this was both as an attention check as well as a way of comparing these findings to previous studies
    - i.e., Connine et al (1991)
--

#### Pos-test
- identical to the pre-test

---

# Visualize then Analyze

```{r visualize}

ggplot(data = test,
       aes(x=vot, y=resp_t, color = condition)) +
  # uncomment to see the actual data points instead of just the mean
  geom_point(position=position_jitter(h = 0.05), alpha=0.1) +
  geom_line(stat="summary", fun = mean) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  facet_wrap( . ~ block) +
  ylab("Proportion of /t/ response") + xlab("VOT (ms)") +
  labs(title = "*Pre- and Post Continuum 'Tent' Responses*", tag = "Figure 1") +
  mdthemes::md_theme_bw() +
  theme(text=element_text(size=11, family = "Times"))

```


---
# Analysis (the important part)
## Bayesian vs. Frequentist analysis of the pre/post-test data

- utilized a Bayesian multi-level model (using brms) and a Frequentist model using (lme4)

- we are utilizing a multi-level logistic regression because...
  - the data is looking at the proportion of /t/-responses (for pre/post tests)
  - and accuracy along with proportion /t/-responses (for exposure)
- multi-level because...
  - we're interested in group-level effects since it's repeated measures and because we want to look at within-participant effects
  
---
# Analysis
## Bayesian vs. Frequentist analysis of the pre/post-test data

.Large[$$\hat{y} = \alpha + {\beta}{X} + {u}{Z} + \epsilon $$]

- recall the equation for MLM


---
# Analysis
## Test data

- Frequentist approach: 

- in order to compare the pre- and post-test, I used lme4 to run the following model: 
  - `glmer(resp_t ~ block * condition * vot + (block | workerId) + (1 | stimulus), data = test, family = "binomial"(link = "logit")`
  
  - taking this apart:
    - glmer: a generali**zed** multi-level regression model 
    - outcome variable: proportion of /t/ responses
    - fixed effects: block (pre vs. post), condition (tent or dent-biasing), and vot
    - random effects: workerId, item, block
      - specifically, random intercepts by item and subject and by subject random slopes for blocks

```{r frequentist, eval=FALSE, include=FALSE}
# convert to probability: logit2prob <- function(logit){
#   odds <- exp(logit)
#   prob <- odds / (1 + odds)
#   return(prob)
# }
# infinite thanks to: https://sebastiansauer.github.io/convert_logit2prob/

# you could add a random intercept for condition, but it may or may not add any real explanatory value. that would technically contribute to like "maximal" random effects structure though

#### Begin Frequentist Modeling

# intercept model 
freq_analysis_int <- glm(resp_t ~ 1, data = test, family = "binomial"(link = "logit"))
summary(freq_analysis_int)

# including interaction and one random effect
freq_analysis_1 <- glmer(resp_t ~ block * condition * vot +
                                  (1 | workerId),
                                  data = test, family = "binomial"(link = "logit"))
# adding one more random effect
freq_analysis_2 <- glmer(resp_t ~ block * condition * vot +
                                  (1 | workerId) +
                                  (1 | stimulus),
                                  data = test, family = "binomial"(link = "logit"), 
                                  control=glmerControl(optimizer="bobyqa",
                                  optCtrl=list(maxfun=2e5)))

# adding in a random slope and lets goooo
freq_analysis_full <- glmer(resp_t ~ block * condition * vot +
                                  (block | workerId) +
                                  (1 | stimulus),
                                  data = test, family = "binomial"(link = "logit"), 
                                  control=glmerControl(optimizer="bobyqa",
                                  optCtrl=list(maxfun=2e5)))
#### Model comparison
freq_anova <-  anova(freq_analysis_1, freq_analysis_2 , freq_analysis_full, test = "Chisq") # well it looks like including stimulus is better?

```

---

# Analysis
## Test data

- Let's take a look at the output
  - things to consider:
      - the vot was centered around the mean
      - the intercept is pre-test and "tent"-biasing
  - significant effects:
    - interaction (yay)
    - vot (expected)
    - intercept (mean: (prob) 0.7267025)
      - the probability of /t/ response in the pre-test and tent-biasing condition given a vot of 50 (remember it's centered around 0 here)
      - which isn't great because around the center should be ambiguous but that's a problem for future me

```{r frequentist-summary}
# Let's take a look at the summary for the full model 
summary(freq_analysis_full)
```


---
# Analysis
# Pre-data

- Bayesian approach: 

- alright, this is very similar, but we're going to make a few adjustments. 
  - `brm(resp_t ~ block * condition * vot + (block | workerId) + (1 | stimulus), data = test, family = "bernoulli"(link = "logit")`
  
```{r bayesian}
#### Bayesian Modeling 

# intercept model
bayes_analysis_int <- brm(resp_t ~ 1, data = test, family = "binomial"(link = "logit"))
summary(bayes_analysis_int)

# adding in one random intercept
bayes_analysis_1 <- brm(resp_t ~ block * condition * vot +
                                  (1 | workerId),
                                  data = test, family = "binomial"(link = "logit"))
summary(bayes_analysis_1)

# adding in another random intercept
bayes_analysis_2 <- brm(resp_t ~ block * condition * vot +
                                  (1 | workerId) +
                                  (1 | stimulus),
                                  data = test, family = "binomial"(link = "logit"))
summary(bayes_analysis_2)

# finally, random intercepts and random slope for block
bayes_analysis_full <- brm(resp_t ~ block * condition * vot +
                                  (block | workerId) +
                                  (1 | stimulus),
                                  data = test, family = "bernoulli"(link = "logit"), 
                                  prior = c(prior("normal(0,0.5)", class = "b"), # setting priors (remember -4 to 4)
                                            prior("normal(0,1", class = "Intercept")), # setting priors (remember -4 to 4)
                                  cores = getOption("mc.cores", 4))

bayes_summary <- summary(bayes_analysis_full)
```


```{r bayesian}
#### Model comparison 
waic(bayes_analysis_2 , bayes_analysis_full) 

prior_summary(bayes_analysis_full)

rstanarm_model <-  rstanarm::stan_glmer(resp_t ~ block * condition * vot +
                                  (block | workerId) +
                                  (1 | stimulus),
                                  data = test, family = "binomial"(link = "logit"))

posterior_table <- as_tibble(bayes_analysis_full)
```

```{r rope-time}
# possible multi-collinearity between b_blockpost.conditiondentMbiasing.vot and b_conditiondentMbiasing.vot (r = 0.75). This might lead to inappropriate results.
ROPE <- rope(bayes_analysis_full,
             range = "default", 
             ci = 0.95, 
             ci_method = "HDI", # or ETI - forgot what the difference is lol
             effects = "all",
             components = "all")
```

```{r parameter-estimates}
# ROPE calculated as: [-0.18, 0.18]
posterior_table %>% 
  select(c("b_Intercept":"b_blockpost:conditiondentMbiasing:vot")) %>% 
  pivot_longer(everything(), names_to = "parameter", values_to = "estimate") %>% 
  ggplot(., aes(x = estimate, y = parameter, fill = stat(abs(x) < .18))) +
    stat_halfeye() + 
    geom_vline(xintercept = c(ROPE$ROPE_low, ROPE$ROPE_high), linetype = "dashed", color = "red") +
    labs(title = "*Parameter estimates for Bayesian Model*", tag = "Figure 2") +
    mdthemes::md_theme_bw() +
    theme(text=element_text(size=11, family = "Times")) + 
    scale_fill_manual(values = c("gray80", "skyblue"))

# not relevant for our current purposes
# mcmc_hist(posterior_table, pars = c("b_Intercept", "b_blockpost:conditiondentMbiasing"), 
#              size = 1.5, alpha = 0.5)


```

```{r posterior-samples}
get_variables(bayes_analysis_full)

bayes_analysis_full %>%
  spread_draws(r_workerId[worker,intercept])
```







